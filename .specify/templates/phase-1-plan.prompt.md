# Planning Prompt: Phase I Todo In-Memory Python Console App

## Project Overview

**Project Name:** Phase I: Todo In-Memory Python Console App - Basic Level Functionality

**Mission:** Build a command-line todo application that stores tasks in memory using spec-driven development practices, demonstrating clean architecture and professional Python project structure for hackathon participants learning Spec-Kit Plus workflows.

**Target Audience:** Developers learning spec-driven development methodology through hands-on practice.

---

## Architecture Sketch

### High-Level System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Console Interface                     â”‚
â”‚                  (User Interaction Layer)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Command Controller                     â”‚
â”‚        (Menu Router & Input Validation Layer)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Task Service Layer                      â”‚
â”‚         (Business Logic & Task Operations)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 In-Memory Repository                     â”‚
â”‚           (Data Storage & Retrieval Layer)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Task Model                           â”‚
â”‚              (Domain Entity & Data Structure)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

1. **Task Model (Domain Layer)**
   - Task entity with id, title, description, status, timestamps
   - Validation rules for task properties
   - Status enumeration (pending, completed)

2. **In-Memory Repository (Data Layer)**
   - Task storage using Python data structures (dict, list)
   - CRUD operations interface
   - ID generation strategy
   - Data retrieval and filtering methods

3. **Task Service (Business Logic Layer)**
   - Orchestrates task operations
   - Implements business rules
   - Error handling and validation
   - Coordinates between repository and controller

4. **Command Controller (Application Layer)**
   - Menu system and user input handling
   - Command routing to service methods
   - Input validation and sanitization
   - Response formatting and display

5. **Console Interface (Presentation Layer)**
   - Menu display and user prompts
   - Output formatting and styling
   - Error message presentation
   - User feedback mechanisms

### Technology Stack

- **Runtime:** Python 3.13+
- **Package Manager:** UV
- **Development Methodology:** Spec-Kit Plus (Spec-Driven Development)
- **Code Assistant:** Claude Code
- **Storage:** In-memory (Python data structures)
- **Interface:** Console/Terminal (stdin/stdout)

### Data Flow Patterns

```
User Input â†’ Validation â†’ Service Logic â†’ Repository â†’ Model
                                                          â”‚
User Output â† Formatting â† Service Response â† Repository â†â”˜
```

---

## Section Structure

Your `plan.md` should follow this organization:

### Phase 1: Research & Foundation
1. **Environment Setup & Tooling**
   - UV package manager configuration
   - Python 3.13+ installation verification
   - Project structure initialization
   - Development workflow setup

2. **Domain Model Research**
   - Task entity attribute design
   - Status lifecycle management
   - Timestamp handling strategies
   - ID generation approaches

3. **Data Structure Selection**
   - In-memory storage options analysis
   - Lookup performance considerations
   - Data structure tradeoffs

### Phase 2: Core Architecture
4. **Module Organization**
   - Package structure design
   - Module responsibility allocation
   - Import dependency management
   - Namespace organization

5. **Repository Pattern Design**
   - Interface definition
   - CRUD operation contracts
   - Error handling strategy
   - Thread safety considerations (if applicable)

6. **Service Layer Design**
   - Business logic encapsulation
   - Validation rules
   - Error propagation strategy
   - Operation orchestration

### Phase 3: Interface & Interaction
7. **Console Interface Design**
   - Menu structure and navigation
   - Input/output formatting
   - User feedback mechanisms
   - Error presentation strategy

8. **Command Controller Architecture**
   - Command routing mechanism
   - Input parsing strategy
   - Validation layer design
   - Response handling

### Phase 4: Quality & Validation
9. **Testing Strategy**
   - Test approach (manual vs automated)
   - Test coverage targets
   - Acceptance criteria validation
   - Edge case identification

10. **Error Handling & Validation**
    - Input validation rules
    - Error taxonomy
    - User-facing error messages
    - Recovery mechanisms

11. **Documentation Requirements**
    - README structure
    - Setup instructions
    - Usage examples
    - CLAUDE.md configuration

---

## Research Approach

### Research-Concurrent Strategy

**DO NOT research everything upfront.** Instead, follow this research-while-planning approach:

1. **Initial Reconnaissance (15 minutes max)**
   - Scan Python project structure conventions
   - Quick review of UV package manager basics
   - Brief survey of console menu patterns

2. **Just-In-Time Research (Throughout Planning)**
   - Research specific topics as you reach them in the plan
   - Document findings immediately in the relevant section
   - Update architecture based on discoveries
   - Iterate on decisions as new information emerges

3. **Research Checkpoints**
   - After domain model design â†’ Research data structure options
   - After repository pattern â†’ Research ID generation strategies
   - After service layer â†’ Research error handling patterns
   - After interface design â†’ Research console formatting libraries

### Specific Research Questions

For each section, answer these targeted questions:

#### 1. Python Project Structure
- What is the recommended package structure for a Python console application?
- How should modules be organized in a UV-managed project?
- What are the best practices for `__init__.py` usage?
- How to structure `pyproject.toml` for UV?

**Sources to Consult:**
- Python Packaging User Guide (packaging.python.org)
- UV documentation (astral.sh/uv)
- PEP 8 Style Guide
- Real Python project structure articles

**Citation Requirement:** Use APA format for all references
- Example: Reitz, K. & Schlusser, T. (2024). The Hitchhiker's Guide to Python. O'Reilly Media.

#### 2. In-Memory Data Structure Design
- What Python data structure is optimal for task storage (dict, list, OrderedDict)?
- How to implement efficient lookup by ID?
- What are the tradeoffs between list and dict for primary storage?
- How to handle ID generation without database auto-increment?

**Research Focus:**
- Time complexity of operations (O(1) vs O(n))
- Memory efficiency
- Code readability
- Scalability to moderate task counts (100-1000 tasks)

**Evidence to Collect:**
- Performance benchmarks for different structures
- Code examples from established projects
- Python documentation on collections module

#### 3. Console Menu Interface Patterns
- What are common patterns for console menu systems in Python?
- Should you use a library (e.g., `questionary`, `simple-term-menu`) or build custom?
- How to handle input validation and error recovery in console apps?
- What are best practices for menu navigation UX?

**Research Questions:**
- Is adding a dependency worth the complexity?
- What does a minimal, clean menu implementation look like?
- How to maintain testability with console I/O?

**Sources:**
- PyPI package comparisons
- GitHub repositories of popular Python CLI tools
- Console UI/UX patterns documentation

#### 4. Task Domain Model
- What attributes are essential for a task entity?
- How to represent task status (enum, string, boolean)?
- Should timestamps be datetime objects or ISO strings?
- What validation rules should the model enforce?

**Research Areas:**
- Python `dataclass` vs `NamedTuple` vs plain class
- `enum.Enum` for status representation
- `datetime` module best practices
- Immutability considerations

#### 5. Repository Pattern Implementation
- How to implement repository pattern in Python without ORM?
- What interface should the repository expose?
- How to handle errors (exceptions vs return codes)?
- Should the repository return copies or original objects?

**Research Focus:**
- Repository pattern examples in Python
- Interface definition strategies (ABC, Protocol)
- Error handling conventions in Python
- Immutability and defensive copying

#### 6. Error Handling Strategy
- What error handling approach fits console applications?
- Should you use custom exceptions or built-in ones?
- How to present errors to users in a friendly way?
- What errors should be recoverable vs fatal?

**Research Topics:**
- Python exception hierarchy
- Custom exception design patterns
- User-facing error message best practices
- Graceful degradation strategies

### Knowledge Gaps to Address

As you plan, explicitly note knowledge gaps and resolve them just-in-time:

- [ ] UV project initialization commands and workflow
- [ ] Python 3.13+ specific features or compatibility considerations
- [ ] Console output formatting options (plain, colored, styled)
- [ ] Testing approaches for console I/O (mocking stdin/stdout)
- [ ] ID generation strategies without external dependencies
- [ ] Task filtering and searching algorithms
- [ ] Input sanitization techniques for console apps

### Validation Criteria for Research

Before accepting a research finding as a basis for decisions:

1. **Authority:** Is the source authoritative (official docs, established experts, peer-reviewed)?
2. **Recency:** Is the information current for Python 3.13+ and modern practices?
3. **Applicability:** Does it apply to console apps and in-memory storage?
4. **Practicality:** Can it be implemented within hackathon constraints?
5. **Evidence:** Are there code examples or benchmarks to support claims?

---

## Decisions Needing Documentation

As you develop the plan, you will encounter architecturally significant decisions. For each decision that meets the significance test (Impact + Alternatives + Scope), suggest an ADR.

### Decision Template

For each major decision point, document:

```markdown
## Decision: [Brief Title]

**Context:** What forces are at play?

**Options Considered:**
1. Option A: [Description]
   - Pros: ...
   - Cons: ...
2. Option B: [Description]
   - Pros: ...
   - Cons: ...

**Tradeoffs:**
- Performance vs Simplicity
- Flexibility vs Constraints
- Development Speed vs Maintainability

**Recommendation:** [Chosen option]

**Rationale:** [Why this option best fits the context]

**Impact Assessment:**
- Scope: [Which components affected?]
- Reversibility: [Easy/Medium/Hard to change later?]
- Cost: [Development time, complexity, dependencies]

**ADR Suggestion:**
ğŸ“‹ Architectural decision detected: [brief description]
   Document reasoning and tradeoffs? Run `/sp.adr [decision-title]`
```

### Expected Significant Decisions

You will likely need to make and document these decisions:

#### 1. Python Project Structure & Module Organization

**Decision:** How to organize modules and packages in the project?

**Options:**
- Flat structure (all modules in `src/`)
- Layered structure (`src/domain/`, `src/service/`, `src/ui/`)
- Feature-based structure (`src/tasks/`)

**Research Needed:**
- Python packaging best practices
- UV project structure conventions
- Maintainability implications

**Impact:** High - affects all future development and imports

**ADR Candidate:** Yes
ğŸ“‹ Architectural decision detected: Python project structure and module organization
   Document reasoning and tradeoffs? Run `/sp.adr project-structure`

---

#### 2. In-Memory Data Structure for Task Storage

**Decision:** What data structure should store tasks in memory?

**Options:**
- Dictionary with task ID as key (`dict[int, Task]`)
- List of tasks with linear search (`list[Task]`)
- OrderedDict for insertion order preservation
- Custom collection class wrapping chosen structure

**Tradeoffs:**
- Lookup speed (O(1) vs O(n))
- Insertion order preservation
- Memory overhead
- Code simplicity

**Research Needed:**
- Performance benchmarks for different scales
- Python collections module capabilities
- Real-world usage patterns in similar apps

**Impact:** High - affects performance and all repository operations

**ADR Candidate:** Yes
ğŸ“‹ Architectural decision detected: In-memory data structure selection for task storage
   Document reasoning and tradeoffs? Run `/sp.adr task-storage-structure`

---

#### 3. ID Generation Strategy

**Decision:** How to generate unique IDs for tasks without a database?

**Options:**
- Sequential integer counter (1, 2, 3, ...)
- UUID4 (universally unique identifiers)
- Timestamp-based IDs
- Hash-based IDs (from title/description)

**Tradeoffs:**
- Simplicity vs uniqueness guarantees
- Human-readability vs collision resistance
- Memory usage (int vs UUID)
- Testability (deterministic vs random)

**Research Needed:**
- Python `uuid` module capabilities
- Counter management strategies
- Collision probability analysis

**Impact:** Medium - affects user experience and data integrity

**ADR Candidate:** Maybe (depends on complexity chosen)

---

#### 4. Task Model Implementation Approach

**Decision:** How to implement the Task entity?

**Options:**
- `@dataclass` (Python 3.7+)
- `NamedTuple` (immutable)
- Plain class with `__init__`
- Pydantic model (adds dependency)

**Tradeoffs:**
- Immutability vs mutability
- Automatic methods (`__repr__`, `__eq__`) vs manual
- Type validation strictness
- External dependencies

**Research Needed:**
- Dataclass features and limitations
- Immutability implications for updates
- Type hinting best practices

**Impact:** Medium - affects all code interacting with tasks

**ADR Candidate:** Maybe (if choosing adds significant complexity)

---

#### 5. Console Menu Implementation

**Decision:** Should we use a library or build a custom menu system?

**Options:**
- Custom implementation (pure Python)
- `questionary` library (interactive prompts)
- `simple-term-menu` library (arrow-key navigation)
- `click` framework (command-based CLI)

**Tradeoffs:**
- Dependency count vs feature richness
- Development time vs polish
- Testing complexity (mocking I/O)
- User experience quality

**Research Needed:**
- Library capabilities and bundle size
- Installation reliability across platforms
- Testing strategies for each approach

**Impact:** Medium - affects user experience and testing

**ADR Candidate:** Yes (if adding dependency)
ğŸ“‹ Architectural decision detected: Console menu implementation approach
   Document reasoning and tradeoffs? Run `/sp.adr console-menu-approach`

---

#### 6. Error Handling & Validation Strategy

**Decision:** How to handle errors and validate input?

**Options:**
- Exceptions for all errors
- Return codes/tuples (success, error_msg)
- Result/Either monad pattern
- Mix of exceptions and validation

**Tradeoffs:**
- Exception overhead vs explicit handling
- Pythonic idioms vs functional patterns
- User experience (error messages)
- Code verbosity

**Research Needed:**
- Python exception best practices
- Console app error UX patterns
- Recovery mechanisms

**Impact:** Medium - affects all user interactions

**ADR Candidate:** Maybe (if choosing unusual pattern)

---

#### 7. Testing Strategy & Approach

**Decision:** What testing approach to use for this hackathon project?

**Options:**
- Manual testing only (console-based)
- Unit tests with mocked I/O
- Integration tests with captured output
- TDD with pytest
- Mix of manual and automated

**Tradeoffs:**
- Development speed vs confidence
- Test maintenance burden
- Hackathon time constraints
- Learning objectives (spec-driven process)

**Research Needed:**
- pytest basics and mocking strategies
- Console I/O testing patterns
- Time investment for test setup

**Impact:** High - affects development workflow

**ADR Candidate:** Yes
ğŸ“‹ Architectural decision detected: Testing strategy for console application
   Document reasoning and tradeoffs? Run `/sp.adr testing-strategy`

---

#### 8. Task Status Representation

**Decision:** How to represent task completion status?

**Options:**
- Boolean flag (`completed: bool`)
- String status (`status: str` with "pending"/"completed")
- Enum (`status: TaskStatus` with enum values)
- Multiple booleans (`is_completed`, `is_archived`, etc.)

**Tradeoffs:**
- Extensibility (future statuses like "in_progress")
- Type safety and validation
- Serialization complexity
- Code readability

**Research Needed:**
- Python enum module usage
- Status lifecycle patterns
- Future-proofing for Phase II

**Impact:** Low-Medium - easy to change but affects display logic

**ADR Candidate:** No (too small, easily reversible)

---

### Decision Significance Test

For each decision above, apply the three-part test:

1. **Impact:** Does it have long-term consequences? (framework, data model, API, architecture)
2. **Alternatives:** Were multiple viable options seriously considered?
3. **Scope:** Is it cross-cutting and influences system design?

**If ALL THREE are true â†’ Suggest ADR**
**If only 1-2 are true â†’ Document in plan but no ADR needed**

---

## Quality Validation

### Acceptance Criteria from Requirements

The plan must address how to validate these Basic Level features:

#### Feature 1: Add Task
- [ ] User can add a task with title and description
- [ ] Task receives unique ID automatically
- [ ] Task is marked as incomplete by default
- [ ] Confirmation message displays with assigned ID
- [ ] Empty title is rejected with error message
- [ ] Description is optional

#### Feature 2: Delete Task
- [ ] User can delete task by ID
- [ ] Successful deletion shows confirmation
- [ ] Invalid ID shows error message
- [ ] Deletion is permanent (no undo)
- [ ] List updates immediately after deletion

#### Feature 3: Update Task
- [ ] User can update task title and/or description by ID
- [ ] Status (complete/incomplete) is preserved during update
- [ ] Invalid ID shows error message
- [ ] Confirmation message shows updated values
- [ ] Empty title during update is rejected

#### Feature 4: View All Tasks
- [ ] List displays all tasks with ID, title, status
- [ ] Completed tasks are visually distinguished (marker or label)
- [ ] Empty list shows friendly message
- [ ] Tasks display in consistent order
- [ ] Description is shown or accessible for each task

#### Feature 5: Mark Complete/Incomplete
- [ ] User can toggle task status by ID
- [ ] Status change is reflected immediately in view
- [ ] Invalid ID shows error message
- [ ] Confirmation message indicates new status
- [ ] Operation is idempotent (marking complete twice is safe)

### Performance Benchmarks

For Phase I in-memory implementation:

- **Operation latency:** All CRUD operations should complete in < 100ms for up to 1000 tasks
- **Startup time:** Application should launch in < 1 second
- **Memory usage:** Reasonable memory footprint (< 50MB for typical usage)

**Note:** These are soft targets; prioritize correctness and code quality over optimization.

### Security Considerations

For this Phase I console app:

- [ ] Input sanitization to prevent code injection (eval, exec)
- [ ] No sensitive data storage (no passwords, tokens)
- [ ] Safe error messages (no stack traces exposed to user)
- [ ] Graceful handling of malformed input

**Out of Scope for Phase I:**
- Authentication/authorization (single-user local app)
- Network security (no network operations)
- Data encryption (in-memory only, no persistence)

### Operational Readiness

#### Setup & Installation
- [ ] README includes step-by-step setup instructions
- [ ] UV installation and project initialization documented
- [ ] Python version requirements clearly stated
- [ ] Dependencies (if any) explicitly listed

#### Observability
- [ ] Error messages are clear and actionable
- [ ] Optional debug/verbose mode for troubleshooting
- [ ] Logging strategy defined (console only, optional file)

#### Documentation
- [ ] README covers all basic operations with examples
- [ ] CLAUDE.md provides context for Claude Code
- [ ] Code comments explain non-obvious design decisions
- [ ] Docstrings for public functions/classes

---

## Testing Strategy

### Test Pyramid Approach

```
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Manual     â”‚  â† Full user workflow testing
       â”‚  Acceptance  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Integration   â”‚  â† Service + Repository interaction
      â”‚     Tests      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚     Unit Tests      â”‚  â† Individual function/method tests
   â”‚  (Model, Service,   â”‚
   â”‚    Repository)      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Testing Phases

#### Phase 1: Define Test Strategy (in plan.md)

Answer these questions:

1. **Manual vs Automated:** What balance fits hackathon constraints?
2. **Test Coverage:** Which components need automated tests?
3. **I/O Mocking:** How to test console interactions without actual I/O?
4. **Test Data:** What sample tasks represent realistic scenarios?

**Decision Needed:** See "Testing Strategy & Approach" decision above.

#### Phase 2: Acceptance Test Scenarios

For each Basic Level feature, define test scenarios:

**Add Task Scenarios:**
1. Happy path: Add task with title and description
2. Edge case: Add task with title only (no description)
3. Error case: Attempt to add task with empty title
4. Boundary: Add task with very long title/description

**Delete Task Scenarios:**
1. Happy path: Delete existing task by valid ID
2. Error case: Attempt to delete with non-existent ID
3. Error case: Attempt to delete with invalid ID format
4. Edge case: Delete last remaining task

**Update Task Scenarios:**
1. Happy path: Update both title and description
2. Edge case: Update title only
3. Edge case: Update description only
4. Error case: Update with empty title
5. Error case: Update non-existent task

**View Tasks Scenarios:**
1. Happy path: View list with mix of completed/incomplete tasks
2. Edge case: View empty list
3. Edge case: View list with only completed tasks
4. Edge case: View list with very long titles

**Mark Complete Scenarios:**
1. Happy path: Mark incomplete task as complete
2. Happy path: Mark complete task as incomplete
3. Idempotent: Mark already-complete task as complete
4. Error case: Mark non-existent task

#### Phase 3: Test Implementation Checklist

- [ ] Repository unit tests (CRUD operations, edge cases)
- [ ] Service layer unit tests (business logic, validation)
- [ ] Model validation tests (if applicable)
- [ ] Integration tests (service + repository)
- [ ] Manual acceptance test script/checklist
- [ ] Error handling tests (invalid input, edge cases)

#### Phase 4: Test Execution & Validation

**Definition of Done:**
- [ ] All unit tests pass
- [ ] All integration tests pass
- [ ] Manual acceptance tests completed successfully
- [ ] All Basic Level features validated against acceptance criteria
- [ ] Error cases handled gracefully
- [ ] Edge cases tested and working

### Test Data Fixtures

Define standard test data for consistent testing:

```python
# Example test fixtures
SAMPLE_TASKS = [
    {"title": "Buy groceries", "description": "Milk, eggs, bread"},
    {"title": "Write docs", "description": ""},
    {"title": "Fix bug #123", "description": "Handle edge case in parser"},
]
```

### I/O Mocking Strategy

**Research Question:** How to test console I/O in Python?

**Options:**
- `unittest.mock.patch` for stdin/stdout
- `pytest` fixtures with `capsys`
- Dependency injection for I/O streams
- Separate UI layer from logic (testable without I/O)

**Recommendation:** Research and decide based on chosen testing framework.

---

## Implementation Checklist

Use this checklist to track progress during implementation (in `tasks.md`):

### Environment & Setup
- [ ] Install/verify Python 3.13+
- [ ] Install UV package manager
- [ ] Initialize UV project structure
- [ ] Configure `pyproject.toml`
- [ ] Set up `.gitignore`
- [ ] Create initial README.md
- [ ] Create CLAUDE.md

### Domain Layer
- [ ] Define Task model/dataclass
- [ ] Implement task validation rules
- [ ] Define status enumeration
- [ ] Add timestamp handling
- [ ] Write model unit tests

### Data Layer
- [ ] Implement in-memory repository
- [ ] Implement ID generation
- [ ] Implement CRUD operations
- [ ] Add error handling
- [ ] Write repository unit tests

### Service Layer
- [ ] Implement task service
- [ ] Add business logic and validation
- [ ] Implement error handling
- [ ] Write service unit tests
- [ ] Write integration tests

### Interface Layer
- [ ] Implement console menu
- [ ] Implement command routing
- [ ] Add input validation
- [ ] Add output formatting
- [ ] Implement error presentation

### Features
- [ ] Feature 1: Add task
- [ ] Feature 2: Delete task
- [ ] Feature 3: Update task
- [ ] Feature 4: View all tasks
- [ ] Feature 5: Mark complete/incomplete

### Quality & Documentation
- [ ] Run all tests
- [ ] Manual acceptance testing
- [ ] Update README with usage examples
- [ ] Add code comments and docstrings
- [ ] Create usage examples
- [ ] Final code review

---

## Risk Analysis

### Top 3 Risks & Mitigation

#### Risk 1: UV Package Manager Unfamiliarity
**Likelihood:** High | **Impact:** Medium
**Description:** Team may not be familiar with UV, causing setup delays.
**Mitigation:**
- Allocate time for UV research and setup (1-2 hours)
- Have fallback to pip/venv if UV issues arise
- Document UV commands in README for future reference
**Kill Switch:** If UV blocks progress > 2 hours, switch to pip/venv

#### Risk 2: Over-Engineering for Phase I
**Likelihood:** Medium | **Impact:** Medium
**Description:** Temptation to add features beyond Basic Level (e.g., persistence, advanced UI).
**Mitigation:**
- Strictly follow spec and acceptance criteria
- Use checklist to track only required features
- Defer enhancements to Phase II planning
**Guardrail:** Review progress against spec every 2 hours

#### Risk 3: Testing Time Underestimation
**Likelihood:** Medium | **Impact:** Low
**Description:** Automated testing setup may take longer than expected in hackathon timeframe.
**Mitigation:**
- Define testing strategy early in planning
- Prioritize manual acceptance tests if time-constrained
- Keep unit tests simple (no complex mocking)
**Kill Switch:** If test setup > 3 hours, pivot to manual testing only

---

## Definition of Done

### Code Quality
- [ ] All code follows PEP 8 style guide
- [ ] No hardcoded values (use constants/config)
- [ ] Proper error handling for all user inputs
- [ ] Clear function/class names and structure
- [ ] Code comments for non-obvious logic
- [ ] No unused imports or dead code

### Functionality
- [ ] All 5 Basic Level features implemented
- [ ] All acceptance criteria met
- [ ] Error cases handled gracefully
- [ ] User feedback for all operations
- [ ] Consistent behavior across features

### Testing
- [ ] Test strategy documented in plan.md
- [ ] All tests passing (if automated testing chosen)
- [ ] Manual acceptance test completed
- [ ] Edge cases validated
- [ ] Error scenarios tested

### Documentation
- [ ] README with clear setup instructions
- [ ] README with usage examples for all features
- [ ] CLAUDE.md with project context
- [ ] Code docstrings for public API
- [ ] Inline comments for complex logic

### Repository
- [ ] Clean git history with meaningful commits
- [ ] Constitution file in place
- [ ] Spec artifacts in specs/ directory
- [ ] No sensitive data or secrets
- [ ] Proper .gitignore

### Spec-Driven Process
- [ ] Constitution created and followed
- [ ] Feature spec written before code
- [ ] Plan document with architectural decisions
- [ ] Tasks broken down with acceptance criteria
- [ ] PHRs created for major milestones
- [ ] ADRs created for significant decisions (if any)

---

## Out of Scope (Explicit Exclusions)

**Phase I explicitly excludes:**

- [ ] Data persistence (file, database, cloud)
- [ ] Multi-user support or concurrency
- [ ] Network operations or APIs
- [ ] GUI or web interface
- [ ] Task filtering, searching, or sorting
- [ ] Task priorities, due dates, or categories
- [ ] Undo/redo functionality
- [ ] Data import/export
- [ ] Advanced formatting (colors, tables, etc.)
- [ ] Configuration files or settings
- [ ] Plugins or extensibility
- [ ] Internationalization (i18n)
- [ ] Performance optimization beyond reasonable limits
- [ ] Deployment or packaging for distribution

**Rationale:** Phase I focuses on core CRUD operations and learning spec-driven development. Advanced features are deferred to Phase II and beyond.

---

## Next Steps After Planning

1. **Review this plan** with team/stakeholders
2. **Create ADRs** for significant decisions identified
3. **Generate `tasks.md`** with detailed implementation tasks
4. **Set up development environment** (UV, Python, repo)
5. **Begin implementation** following task order
6. **Create PHRs** for each major milestone
7. **Test incrementally** as features are completed
8. **Document** as you build, not as an afterthought

---

## Research Resources & Citations

As you research, document sources here using APA format:

### Python Project Structure
- Python Packaging Authority. (2024). Python Packaging User Guide. https://packaging.python.org/

### UV Package Manager
- Astral. (2024). UV: An extremely fast Python package installer and resolver. https://astral.sh/uv

### Console Application Patterns
- [Add citations as you research]

### Data Structure Design
- [Add citations as you research]

### Repository Pattern
- [Add citations as you research]

### Testing Strategies
- [Add citations as you research]

---

## Appendix: Quick Reference

### UV Commands (to be verified during research)
```bash
# Initialize project
uv init

# Add dependency
uv add <package>

# Run application
uv run python src/main.py

# Run tests
uv run pytest
```

### Project Structure (Proposed, subject to research)
```
todo/
â”œâ”€â”€ .specify/
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â””â”€â”€ constitution.md
â”‚   â””â”€â”€ templates/
â”œâ”€â”€ specs/
â”‚   â””â”€â”€ phase-1/
â”‚       â”œâ”€â”€ spec.md
â”‚       â”œâ”€â”€ plan.md
â”‚       â””â”€â”€ tasks.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ task.py
â”‚   â”œâ”€â”€ repository/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ task_repository.py
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ task_service.py
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ console.py
â”‚       â””â”€â”€ controller.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_domain.py
â”‚   â”œâ”€â”€ test_repository.py
â”‚   â”œâ”€â”€ test_service.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ CLAUDE.md
â””â”€â”€ pyproject.toml
```

---

## Planning Session Workflow

When you use this prompt to create `plan.md`, follow this flow:

1. **Start with Architecture Sketch** (15 min)
   - Draw out the layers and components
   - Identify data flow
   - Note technology choices

2. **Quick Initial Research** (15 min)
   - Scan UV docs for basic commands
   - Quick review of Python project structure
   - Identify any immediate blockers

3. **Work Through Sections Sequentially** (2-4 hours)
   - Research just-in-time as you reach each section
   - Document findings immediately
   - Make decisions with rationale
   - Note ADR candidates

4. **Identify & Document Decisions** (30 min)
   - Review all decisions made
   - Apply significance test
   - Suggest ADRs for qualifying decisions
   - Document tradeoffs

5. **Define Acceptance Criteria** (30 min)
   - Translate requirements to testable criteria
   - Define test scenarios
   - Establish Definition of Done

6. **Risk & Validation** (15 min)
   - Identify top risks
   - Define mitigation strategies
   - Set quality checkpoints

7. **Review & Refine** (15 min)
   - Ensure all sections complete
   - Check for consistency
   - Validate against requirements
   - Add citations in APA format

**Total Planning Time:** 4-6 hours for thorough planning

---

## Success Criteria for This Plan

Your `plan.md` is successful if:

- [ ] All architectural components are clearly defined
- [ ] All significant decisions are documented with rationale
- [ ] ADR suggestions are made for qualifying decisions
- [ ] Research findings are cited in APA format
- [ ] Acceptance criteria cover all Basic Level features
- [ ] Testing strategy is clearly defined
- [ ] Risks are identified with mitigation plans
- [ ] Out of scope items are explicitly listed
- [ ] Definition of Done is comprehensive
- [ ] Plan is actionable and can drive task generation
- [ ] Someone unfamiliar with the project could understand the architecture
- [ ] The plan balances detail with readability

---

**Remember:** This is a learning project. The process is as important as the product. Document your reasoning, make thoughtful decisions, and follow the spec-driven development methodology. The goal is not just a working todo app, but a clear demonstration of professional software development practices.

Good luck with your hackathon!
